{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vndreddy6/Colab_Notebooks/blob/master/SVM_Case_Study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Date Fruit Varieties with Support Vector Machines\n",
        "\n",
        "### Context\n",
        "\n",
        "Welcome to the **Scaler Agricultural Analytics** team! Our objective is to leverage advanced machine learning techniques to predict the variety of date fruits, empowering farmers and agricultural stakeholders to improve classification accuracy and streamline post-harvest processes. Your role is to analyze various morphological, colorimetric, and textural attributes of date fruits to build predictive models that distinguish different varieties effectively.\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "You have been provided with a comprehensive dataset containing morphological and colorimetric features of different varieties of date fruits. The dataset includes the following attributes:\n",
        "\n",
        "### Morphological Attributes:\n",
        "- **AREA:** Surface area of the date fruit.\n",
        "- **PERIMETER:** Perimeter measurement around the fruit.\n",
        "- **MAJOR_AXIS:** Length of the major axis of the date fruit.\n",
        "- **MINOR_AXIS:** Length of the minor axis of the date fruit.\n",
        "- **ECCENTRICITY:** Ratio describing the shape of the fruit based on the axes.\n",
        "- **EQDIASQ:** Equivalent diameter of a circle with the same area as the fruit.\n",
        "- **SOLIDITY:** Ratio of the area to the convex hull area.\n",
        "- **CONVEX_AREA:** Area of the smallest convex polygon that can contain the fruit.\n",
        "- **EXTENT:** Ratio of the area to the bounding box area.\n",
        "- **ASPECT_RATIO:** Ratio of the major axis to the minor axis.\n",
        "- **ROUNDNESS:** Measure of how circular the fruit is.\n",
        "- **COMPACTNESS:** Measure of how compact or dense the fruit is.\n",
        "\n",
        "### Shape Factor Attributes:\n",
        "- **SHAPEFACTOR_1:** Ratio of the perimeter squared to 4π times the area.\n",
        "- **SHAPEFACTOR_2:** Ratio of 4π times the area to the perimeter squared.\n",
        "- **SHAPEFACTOR_3:** Ratio of the major axis to the equivalent diameter.\n",
        "- **SHAPEFACTOR_4:** Ratio of the minor axis to the equivalent diameter.\n",
        "\n",
        "### Colorimetric Attributes:\n",
        "- **MeanRR:** Mean intensity of the red color channel.\n",
        "- **MeanRG:** Mean intensity of the green color channel.\n",
        "- **MeanRB:** Mean intensity of the blue color channel.\n",
        "- **StdDevRR:** Standard deviation of the red color channel.\n",
        "- **StdDevRG:** Standard deviation of the green color channel.\n",
        "- **StdDevRB:** Standard deviation of the blue color channel.\n",
        "- **SkewRR:** Skewness of the red color channel.\n",
        "- **SkewRG:** Skewness of the green color channel.\n",
        "- **SkewRB:** Skewness of the blue color channel.\n",
        "- **KurtosisRR:** Kurtosis of the red color channel.\n",
        "- **KurtosisRG:** Kurtosis of the green color channel.\n",
        "- **KurtosisRB:** Kurtosis of the blue color channel.\n",
        "- **EntropyRR:** Entropy of the red color channel.\n",
        "- **EntropyRG:** Entropy of the green color channel.\n",
        "- **EntropyRB:** Entropy of the blue color channel.\n",
        "\n",
        "### Daubechies Wavelet Attributes:\n",
        "- **ALLdaub4RR:** Wavelet-transformed feature of the red color channel.\n",
        "- **ALLdaub4RG:** Wavelet-transformed feature of the green color channel.\n",
        "- **ALLdaub4RB:** Wavelet-transformed feature of the blue color channel.\n",
        "\n",
        "### Target Attribute:\n",
        "- **Class:** The variety or class of the date fruit.\n",
        "- Check this [Date Fruit Vlog](https://www.liveeatlearn.com/types-of-dates/) to see the images of these dates\n",
        "\n",
        "Your task is to utilize Support Vector Machines (SVMs) to predict the \"Class\" of each date fruit and identify the most influential features contributing to accurate classification. This project will help streamline the sorting and grading processes in the agricultural industry, offering practical insights to farmers and other stakeholders.\n",
        "\n"
      ],
      "metadata": {
        "id": "YvzLFyMDGYaV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljTtaTotxMcJ",
        "outputId": "e21f724f-8795-4eaf-fb8b-84642850ae72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-06 04:50:05--  https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/070/671/original/Date_Fruit_Datasets.zip\n",
            "Resolving d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)... 13.224.9.181, 13.224.9.24, 13.224.9.129, ...\n",
            "Connecting to d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)|13.224.9.181|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 416324 (407K) [application/zip]\n",
            "Saving to: ‘Date_Fruit_Datasets.zip’\n",
            "\n",
            "Date_Fruit_Datasets 100%[===================>] 406.57K   469KB/s    in 0.9s    \n",
            "\n",
            "2024-05-06 04:50:07 (469 KB/s) - ‘Date_Fruit_Datasets.zip’ saved [416324/416324]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/070/671/original/Date_Fruit_Datasets.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Date_Fruit_Datasets.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk54DLcFxY3z",
        "outputId": "2d92452a-a9bc-42df-bf7e-24c80bae83af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Date_Fruit_Datasets.zip\n",
            "   creating: Date_Fruit_Datasets/\n",
            "  inflating: Date_Fruit_Datasets/Date_Fruit_Datasets.arff  \n",
            "  inflating: Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx  \n",
            "  inflating: Date_Fruit_Datasets/Date_Fruit_Datasets_Citation_Request.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "6Ts33OT-xbqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('/content/Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx')"
      ],
      "metadata": {
        "id": "9ybn3sJlxdXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "fJXCw6ftxebM",
        "outputId": "7bee9922-1e95-4503-fbfb-ce4b77f4cbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n",
              "0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n",
              "1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n",
              "2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n",
              "3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n",
              "4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n",
              "\n",
              "   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n",
              "0    0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n",
              "1    0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n",
              "2    0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n",
              "3    0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n",
              "4    0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n",
              "\n",
              "   KurtosisRB    EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  ALLdaub4RG  \\\n",
              "0      4.2287 -59191263232 -50714214400 -39922372608     58.7255     54.9554   \n",
              "1      3.1704 -34233065472 -37462601728 -31477794816     50.0259     52.8168   \n",
              "2      4.7192 -93948354560 -74738221056 -60311207936     65.4772     59.2860   \n",
              "3      8.2618 -32074307584 -32060925952 -29575010304     43.3900     44.1259   \n",
              "4      4.4146 -39980974080 -35980042240 -25593278464     52.7743     50.9080   \n",
              "\n",
              "   ALLdaub4RB  Class  \n",
              "0     47.8400  BERHI  \n",
              "1     47.8315  BERHI  \n",
              "2     51.9378  BERHI  \n",
              "3     41.1882  BERHI  \n",
              "4     42.6666  BERHI  \n",
              "\n",
              "[5 rows x 35 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c078c7ce-e4fe-4d42-9af3-8964295323ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AREA</th>\n",
              "      <th>PERIMETER</th>\n",
              "      <th>MAJOR_AXIS</th>\n",
              "      <th>MINOR_AXIS</th>\n",
              "      <th>ECCENTRICITY</th>\n",
              "      <th>EQDIASQ</th>\n",
              "      <th>SOLIDITY</th>\n",
              "      <th>CONVEX_AREA</th>\n",
              "      <th>EXTENT</th>\n",
              "      <th>ASPECT_RATIO</th>\n",
              "      <th>...</th>\n",
              "      <th>KurtosisRR</th>\n",
              "      <th>KurtosisRG</th>\n",
              "      <th>KurtosisRB</th>\n",
              "      <th>EntropyRR</th>\n",
              "      <th>EntropyRG</th>\n",
              "      <th>EntropyRB</th>\n",
              "      <th>ALLdaub4RR</th>\n",
              "      <th>ALLdaub4RG</th>\n",
              "      <th>ALLdaub4RB</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>422163</td>\n",
              "      <td>2378.908</td>\n",
              "      <td>837.8484</td>\n",
              "      <td>645.6693</td>\n",
              "      <td>0.6373</td>\n",
              "      <td>733.1539</td>\n",
              "      <td>0.9947</td>\n",
              "      <td>424428</td>\n",
              "      <td>0.7831</td>\n",
              "      <td>1.2976</td>\n",
              "      <td>...</td>\n",
              "      <td>3.2370</td>\n",
              "      <td>2.9574</td>\n",
              "      <td>4.2287</td>\n",
              "      <td>-59191263232</td>\n",
              "      <td>-50714214400</td>\n",
              "      <td>-39922372608</td>\n",
              "      <td>58.7255</td>\n",
              "      <td>54.9554</td>\n",
              "      <td>47.8400</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>338136</td>\n",
              "      <td>2085.144</td>\n",
              "      <td>723.8198</td>\n",
              "      <td>595.2073</td>\n",
              "      <td>0.5690</td>\n",
              "      <td>656.1464</td>\n",
              "      <td>0.9974</td>\n",
              "      <td>339014</td>\n",
              "      <td>0.7795</td>\n",
              "      <td>1.2161</td>\n",
              "      <td>...</td>\n",
              "      <td>2.6228</td>\n",
              "      <td>2.6350</td>\n",
              "      <td>3.1704</td>\n",
              "      <td>-34233065472</td>\n",
              "      <td>-37462601728</td>\n",
              "      <td>-31477794816</td>\n",
              "      <td>50.0259</td>\n",
              "      <td>52.8168</td>\n",
              "      <td>47.8315</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>526843</td>\n",
              "      <td>2647.394</td>\n",
              "      <td>940.7379</td>\n",
              "      <td>715.3638</td>\n",
              "      <td>0.6494</td>\n",
              "      <td>819.0222</td>\n",
              "      <td>0.9962</td>\n",
              "      <td>528876</td>\n",
              "      <td>0.7657</td>\n",
              "      <td>1.3150</td>\n",
              "      <td>...</td>\n",
              "      <td>3.7516</td>\n",
              "      <td>3.8611</td>\n",
              "      <td>4.7192</td>\n",
              "      <td>-93948354560</td>\n",
              "      <td>-74738221056</td>\n",
              "      <td>-60311207936</td>\n",
              "      <td>65.4772</td>\n",
              "      <td>59.2860</td>\n",
              "      <td>51.9378</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>416063</td>\n",
              "      <td>2351.210</td>\n",
              "      <td>827.9804</td>\n",
              "      <td>645.2988</td>\n",
              "      <td>0.6266</td>\n",
              "      <td>727.8378</td>\n",
              "      <td>0.9948</td>\n",
              "      <td>418255</td>\n",
              "      <td>0.7759</td>\n",
              "      <td>1.2831</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0401</td>\n",
              "      <td>8.6136</td>\n",
              "      <td>8.2618</td>\n",
              "      <td>-32074307584</td>\n",
              "      <td>-32060925952</td>\n",
              "      <td>-29575010304</td>\n",
              "      <td>43.3900</td>\n",
              "      <td>44.1259</td>\n",
              "      <td>41.1882</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>347562</td>\n",
              "      <td>2160.354</td>\n",
              "      <td>763.9877</td>\n",
              "      <td>582.8359</td>\n",
              "      <td>0.6465</td>\n",
              "      <td>665.2291</td>\n",
              "      <td>0.9908</td>\n",
              "      <td>350797</td>\n",
              "      <td>0.7569</td>\n",
              "      <td>1.3108</td>\n",
              "      <td>...</td>\n",
              "      <td>2.7016</td>\n",
              "      <td>2.9761</td>\n",
              "      <td>4.4146</td>\n",
              "      <td>-39980974080</td>\n",
              "      <td>-35980042240</td>\n",
              "      <td>-25593278464</td>\n",
              "      <td>52.7743</td>\n",
              "      <td>50.9080</td>\n",
              "      <td>42.6666</td>\n",
              "      <td>BERHI</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 35 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c078c7ce-e4fe-4d42-9af3-8964295323ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c078c7ce-e4fe-4d42-9af3-8964295323ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c078c7ce-e4fe-4d42-9af3-8964295323ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c9de56bd-fc57-4bc5-bb5e-7e3c5baeae35\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9de56bd-fc57-4bc5-bb5e-7e3c5baeae35')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c9de56bd-fc57-4bc5-bb5e-7e3c5baeae35 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJJ2P5qkxfxU",
        "outputId": "786192df-0ee7-49cf-e6c3-fa37f68d4996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 898 entries, 0 to 897\n",
            "Data columns (total 35 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   AREA           898 non-null    int64  \n",
            " 1   PERIMETER      898 non-null    float64\n",
            " 2   MAJOR_AXIS     898 non-null    float64\n",
            " 3   MINOR_AXIS     898 non-null    float64\n",
            " 4   ECCENTRICITY   898 non-null    float64\n",
            " 5   EQDIASQ        898 non-null    float64\n",
            " 6   SOLIDITY       898 non-null    float64\n",
            " 7   CONVEX_AREA    898 non-null    int64  \n",
            " 8   EXTENT         898 non-null    float64\n",
            " 9   ASPECT_RATIO   898 non-null    float64\n",
            " 10  ROUNDNESS      898 non-null    float64\n",
            " 11  COMPACTNESS    898 non-null    float64\n",
            " 12  SHAPEFACTOR_1  898 non-null    float64\n",
            " 13  SHAPEFACTOR_2  898 non-null    float64\n",
            " 14  SHAPEFACTOR_3  898 non-null    float64\n",
            " 15  SHAPEFACTOR_4  898 non-null    float64\n",
            " 16  MeanRR         898 non-null    float64\n",
            " 17  MeanRG         898 non-null    float64\n",
            " 18  MeanRB         898 non-null    float64\n",
            " 19  StdDevRR       898 non-null    float64\n",
            " 20  StdDevRG       898 non-null    float64\n",
            " 21  StdDevRB       898 non-null    float64\n",
            " 22  SkewRR         898 non-null    float64\n",
            " 23  SkewRG         898 non-null    float64\n",
            " 24  SkewRB         898 non-null    float64\n",
            " 25  KurtosisRR     898 non-null    float64\n",
            " 26  KurtosisRG     898 non-null    float64\n",
            " 27  KurtosisRB     898 non-null    float64\n",
            " 28  EntropyRR      898 non-null    int64  \n",
            " 29  EntropyRG      898 non-null    int64  \n",
            " 30  EntropyRB      898 non-null    int64  \n",
            " 31  ALLdaub4RR     898 non-null    float64\n",
            " 32  ALLdaub4RG     898 non-null    float64\n",
            " 33  ALLdaub4RB     898 non-null    float64\n",
            " 34  Class          898 non-null    object \n",
            "dtypes: float64(29), int64(5), object(1)\n",
            "memory usage: 245.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = df.Class.value_counts()\n",
        "classes = list(class_counts.index)\n",
        "n_classes = len(classes)\n",
        "print(f\"we've got {n_classes} classes\\n\")\n",
        "display(class_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "lIRYxuBxxita",
        "outputId": "c14c3e33-aa2f-4a2b-a8dd-2b7999355253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we've got 7 classes\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Class\n",
              "DOKOL     204\n",
              "SAFAVI    199\n",
              "ROTANA    166\n",
              "DEGLET     98\n",
              "SOGAY      94\n",
              "IRAQI      72\n",
              "BERHI      65\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Assesment SVM"
      ],
      "metadata": {
        "id": "nDeKMF_ZzBQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Feature Correlation Analysis\n",
        "\n",
        "#### Context:\n",
        "Support Vector Machines (SVM) are sensitive to highly correlated features, especially when using linear kernels. Such correlations can negatively affect the model’s generalization ability and potentially lead to overfitting.\n",
        "\n",
        "#### Task:\n",
        "Calculate the Pearson correlation coefficients among 'PERIMETER', 'MAJOR_AXIS', 'MINOR_AXIS', and 'COMPACTNESS'. These insights will help in understanding the interdependencies among features that are critical to SVM performance.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Calculate Correlation Matrix:** Use the dataset to calculate the Pearson correlation matrix for the specified features.\n",
        "2. **Visualize Correlation Matrix:** Create a heatmap to visualize the correlations between features, which will help identify which pairs are most correlated.\n",
        "\n",
        "#### Question:\n",
        "Based on the heatmap, which pair of features shows the highest correlation? Discuss the potential impact of this correlation on SVM classification and suggest how to mitigate negative effects.\n",
        "\n",
        "#### Options:\n",
        "A) 'PERIMETER' and 'MAJOR_AXIS'  \n",
        "B) 'MAJOR_AXIS' and 'MINOR_AXIS'  \n",
        "C) 'MINOR_AXIS' and 'COMPACTNESS'  \n",
        "D) 'PERIMETER' and 'COMPACTNESS'\n",
        "\n"
      ],
      "metadata": {
        "id": "1a-XYA6wzE9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Features to include in the correlation matrix\n",
        "features = ['PERIMETER', 'MAJOR_AXIS', 'MINOR_AXIS', 'COMPACTNESS']\n",
        "# TODO: Compute the correlation matrix for the selected features\n",
        "correlation_matrix = _________\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "# TODO: Fill in the appropriate variable to visualize the correlation matrix\n",
        "sns.heatmap(________, annot=True, cmap='coolwarm', linewidths=2, linecolor='black')\n",
        "plt.title('Heatmap of Pearson Correlation Among Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oZ5Ds0LfxnF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluating SVM Precision with Feature Scaling\n",
        "\n",
        "#### Context:\n",
        "Support Vector Machine (SVM) is sensitive to the scale of the data, which can significantly impact its performance. This question aims to explore the effect of feature scaling on SVM classification accuracy and precision for each class.\n",
        "\n",
        "#### Task:\n",
        "After applying feature scaling, identify the class with the lowest precision in an SVM classification task. The SVM model uses a linear kernel.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Prepare Data:**\n",
        "   - Split the dataset into training and test sets using `train_test_split` with `test_size=0.3` and `random_state=42`.\n",
        "   - Extract features (`X`) and the target variable (`y`) from the dataset.\n",
        "\n",
        "2. **Feature Scaling:**\n",
        "   - Scale the features using `StandardScaler`.\n",
        "\n",
        "3. **Train SVM:**\n",
        "   - Train a Support Vector Machine with a linear kernel on the scaled training data.\n",
        "\n",
        "4. **Evaluate the Model:**\n",
        "   - Predict the target variable on the scaled test set.\n",
        "   - Generate a classification report to review the precision scores for each class.\n",
        "\n",
        "\n",
        "#### Question:\n",
        "After training an SVM with a linear kernel on scaled data, which class in the classification report exhibits the lowest precision?\n",
        "\n",
        "#### Options:\n",
        "(A) SAFAVI\n",
        "\n",
        "(B) IRAQI\n",
        "\n",
        "(C) ROTANA\n",
        "\n",
        "(D) DEGLET\n"
      ],
      "metadata": {
        "id": "qUeyQh1I2cj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.___ import SVC  # TODO: Import the SVC (Support Vector Classifier) from sklearn.svm\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preparing the feature and target variables\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initializing the SVM model\n",
        "svm = SVC(_____='linear')  # TODO: Specify the kernel type for the SVC model\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.____(X_train)  # TODO: Apply the scaling to the training features\n",
        "X_test_scaled = scaler.____(X_test)  # TODO: Apply the scaling to the test features\n",
        "\n",
        "# Training the SVM model\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "predictions_scaled = svm.____(X_test_scaled)  # TODO: Use the model to predict the test set\n",
        "\n",
        "print(\"With Scaling:\")\n",
        "print(classification_report(____, ____))  # TODO: Provide the true and predicted values to generate the classification report"
      ],
      "metadata": {
        "id": "89XNYDWt0nPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Optimal Feature Count for Highest Precision\n",
        "\n",
        "#### Context:\n",
        "Feature selection is crucial in machine learning to reduce dimensionality and improve model performance. This exercise involves using Recursive Feature Elimination (RFE) with a Support Vector Machine (SVM) to identify the optimal number of features that yield the highest precision.\n",
        "\n",
        "#### Task:\n",
        "Analyze the effect of different numbers of features on the precision of an SVM model trained with these features. Identify the number of features that leads to the highest precision.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Data Preparation:**\n",
        "   - Load the dataset and separate it into features (`X`) and the target variable (`y`).\n",
        "   - Standardize the features to improve model training.\n",
        "\n",
        "2. **Feature Selection and Model Training:**\n",
        "   - Implement Recursive Feature Elimination (RFE) with an SVM classifier to select varying numbers of features.\n",
        "   - Train an SVM model using these selected features.\n",
        "   - Use `StratifiedKFold` for cross-validation to ensure the model is robust and generalizable.\n",
        "\n",
        "3. **Evaluation:**\n",
        "   - Calculate cross-validated precision for the model with different numbers of selected features.\n",
        "   - Record and plot precision against the number of features.\n",
        "\n",
        "4. **Analysis:**\n",
        "   - Identify the number of features for which the precision is maximized.\n",
        "\n",
        "\n",
        "#### Question:\n",
        "Based on the evaluation, what is the number of features required to achieve the highest precision with the SVM model?\n",
        "\n",
        "#### Options:\n",
        "(A) 5 features  \n",
        "(B) 8 features  \n",
        "(C) 16 features  \n",
        "(D) 18 features  \n",
        "\n",
        "\n",
        "Note: Read about [Recursive feature elimination ](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) here to learn more about how it works."
      ],
      "metadata": {
        "id": "9hQ7QiIE8TUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Load your data and separate into X (features) and y (target)\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Prepare cross-validation (use StratifiedKFold for classification tasks)\n",
        "cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "# Lists to store metrics\n",
        "accuracies = []\n",
        "precisions = []\n",
        "f1_scores = []\n",
        "feature_counts = ______  # TODO: Define the range of feature counts to be selected\n",
        "\n",
        "for n in feature_counts:\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.______(X)     # TODO: Apply scaling to the feature set\n",
        "\n",
        "    # Feature selection\n",
        "    selector = RFE(SVC(kernel='linear', random_state=42), n_features_to_select=_____, step=1)   # TODO: Specify the number of features to select\n",
        "    X_selected = _____.____(X_scaled, y)    # TODO: Fit and transform the data with the selector\n",
        "\n",
        "    # SVM classifier\n",
        "      ___(kernel='linear', random_state=10)   # TODO: Initialize the SVM model with appropriate parameters\n",
        "\n",
        "    # Evaluate using cross-validation\n",
        "    accuracy_scores = []\n",
        "    precision_scores = []\n",
        "    f1_scores_list = []\n",
        "    for train_index, test_index in cv.split(X_selected, y):\n",
        "        X_train_cv, X_test_cv = X_selected[train_index], X_selected[test_index]\n",
        "        y_train_cv, y_test_cv = y[train_index], y[test_index]\n",
        "\n",
        "        svm.fit(______, y_train_cv)    # TODO: Fit the SVM model on the training data\n",
        "        y_pred = svm.______(X_test_cv)   # TODO: Make predictions on the test data\n",
        "\n",
        "        accuracy_scores.append(_____(y_test_cv, y_pred))    # TODO: Calculate the accuracy score\n",
        "        precision_scores.append(_____(y_test_cv, y_pred, average='weighted'))   # TODO: Calculate the precision score\n",
        "        f1_scores_list.append(_____(y_test_cv, y_pred, average='weighted'))    # TODO: Calculate the F1 score\n",
        "\n",
        "    accuracies.append(np.mean(accuracy_scores))\n",
        "    precisions.append(np.mean(precision_scores))\n",
        "    f1_scores.append(np.mean(f1_scores_list))\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(feature_counts, accuracies, label='Accuracy', marker='o')\n",
        "plt.plot(feature_counts, precisions, label='Precision', marker='o')\n",
        "plt.plot(feature_counts, f1_scores, label='F1 Score', marker='o')\n",
        "\n",
        "# Highlight the point with highest precision\n",
        "max_precision_index = np.argmax(precisions)\n",
        "max_precision_feature_count = feature_counts[max_precision_index]\n",
        "plt.scatter(max_precision_feature_count, precisions[max_precision_index], color='red')\n",
        "plt.axvline(x=max_precision_feature_count, color='r', linestyle='--', lw=2)\n",
        "plt.annotate(f'Highest Precision: {precisions[max_precision_index]:.2f} at {max_precision_feature_count} features',\n",
        "             (max_precision_feature_count, precisions[max_precision_index]),\n",
        "             textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "plt.xlabel('Number of Features Selected')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Cross-Validated Performance Metrics vs. Number of Features')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_MCrggcxlgTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Optimal Degree for Polynomial\n",
        "\n",
        "#### Context:\n",
        "Support Vector Machines (SVM) with a polynomial kernel are useful for non-linear data classification. The degree of the polynomial kernel plays a critical role in the model's ability to capture complex patterns in the data. This exercise involves determining the optimal polynomial degree for maximum precision in an SVM model.\n",
        "\n",
        "#### Task:\n",
        "Evaluate the performance of an SVM with a polynomial kernel at various degrees and identify the degree that results in the highest precision.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Data Preparation:**\n",
        "   - Assume `X_train_scaled` and `y_train` are your scaled feature set and target variable, respectively, prepared for training.\n",
        "\n",
        "2. **Model Configuration and Evaluation:**\n",
        "   - Set up a polynomial kernel SVM for degrees [2, 3, 4, 5].\n",
        "   - Use cross-validation to evaluate the accuracy, precision, and F1 score for each degree.\n",
        "   - Collect and plot these metrics to visualize how they vary with the polynomial degree.\n",
        "\n",
        "3. **Analysis:**\n",
        "   - Identify which polynomial degree achieves the highest precision score.\n",
        "\n",
        "#### Question:\n",
        "Based on the cross-validated performance metrics, which degree of the polynomial kernel results in the highest precision?\n",
        "\n",
        "#### Options:\n",
        "(A) Degree 2  \n",
        "(B) Degree 3  \n",
        "(C) Degree 4  \n",
        "(D) Degree 5  \n",
        "\n",
        "\n",
        "#### Learning Point:\n",
        "This exercise highlights the impact of hyperparameter tuning on the effectiveness of machine learning algorithms, particularly how the degree of the polynomial kernel influences the precision of classifications in SVM. This helps students in practical settings where choosing the right model configuration is crucial for achieving the best results."
      ],
      "metadata": {
        "id": "KYtTg8Zv8ZQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Prepare cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Testing different degrees of the polynomial kernel\n",
        "degrees = [2, 3, 4, 5]\n",
        "accuracies = []\n",
        "precisions = []\n",
        "f1_scores = []\n",
        "\n",
        "for degree in degrees:\n",
        "    svm_poly = SVC(kernel=_____, ____=degree, gamma='scale', random_state=10)  # TODO: Specify the kernel type and degree parameter\n",
        "\n",
        "    # Cross-validate the model\n",
        "    accuracy = ______(svm_poly, X_train_scaled, y_train, cv=cv, scoring='accuracy').mean()  # TODO: Use the appropriate cross-validation function to get the mean accuracy\n",
        "    precision = cross_val_score(svm_poly, ______, y_train, cv=cv, scoring='precision_weighted').____()  # TODO: Provide the feature set and call the appropriate method to get the mean precision\n",
        "    f1 = cross_val_score(svm_poly, X_train_scaled, y_train, ____=cv, ______='f1_weighted').mean()  # TODO: Specify the appropriate parameters for cross-validation\n",
        "\n",
        "    # Store results for plotting\n",
        "    accuracies.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(degrees, accuracies, label='Accuracy', marker='o')\n",
        "plt.plot(degrees, precisions, label='Precision', marker='o')\n",
        "plt.plot(degrees, f1_scores, label='F1 Score', marker='o')\n",
        "plt.xlabel('Degree of Polynomial Kernel')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Performance Metrics for SVM with Polynomial Kernel')\n",
        "plt.xticks(degrees)  # Set x-ticks to be the degrees for better readability\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W9f-Xrpi7SeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Determining Optimal Gamma for SVM with RBF Kernel\n",
        "\n",
        "#### Context:\n",
        "In Support Vector Machines (SVM) using the radial basis function (RBF) kernel, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'. Adjusting gamma can significantly affect the model's ability to generalize.\n",
        "\n",
        "#### Task:\n",
        "Identify the optimal gamma value for an SVM model using the RBF kernel that results in the highest precision. Gamma values are tested on a logarithmic scale from \\($10^{-4}$\\) to \\($10^1$\\).\n",
        "\n",
        "#### Instructions:\n",
        "1. **Setup and Data Preparation:**\n",
        "   - Use `StratifiedKFold` for cross-validation with 5 splits, shuffled data, and a set random state for reproducibility.\n",
        "   - Standardize the features to improve model performance.\n",
        "   - Prepare a range of gamma values on a logarithmic scale for testing.\n",
        "\n",
        "2. **Model Training and Evaluation:**\n",
        "   - Train an SVM model with the RBF kernel at various gamma settings.\n",
        "   - Use cross-validation to evaluate accuracy, precision, and F1 score for each gamma value.\n",
        "   - Record these scores for analysis.\n",
        "\n",
        "3. **Analysis and Visualization:**\n",
        "   - Plot accuracy, precision, and F1 score against gamma values.\n",
        "   - Highlight the gamma value that results in the highest precision.\n",
        "   - Use logarithmic scaling for the gamma axis to enhance visualization.\n",
        "\n",
        "\n",
        "\n",
        "#### Question:\n",
        "Based on the evaluation, in which range does the optimal gamma value for achieving the highest precision fall?\n",
        "\n",
        "#### Options:\n",
        "(A) \\($10^{-4}$\\) to \\($10^{-3}$\\)  \n",
        "(B) \\($10^{-3}$\\) to \\($10^{-2}$\\)  \n",
        "(C) \\($10^{-2}$\\) to \\($10^{-1}$\\)  \n",
        "(D) \\($10^{0}$\\) to \\($10^{1}$\\)\n",
        "\n",
        "\n",
        "#### Learning Point:\n",
        "This question teaches the importance of tuning hyperparameters in kernel-based models like SVMs, demonstrating how gamma impacts the model's ability to generalize from the training data to unseen data. This knowledge is crucial for effectively applying SVMs to real-world classification problems.\n"
      ],
      "metadata": {
        "id": "JNdvL1DA_OVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Testing a new range of gamma values with more granularity\n",
        "gamma_values = np.logspace(-4, 1, 20)  # Logarithmic scale from 10^-4 to 10^1\n",
        "accuracies = []\n",
        "precisions = []\n",
        "f1_scores = []\n",
        "\n",
        "for gamma in gamma_values:\n",
        "    svm_rbf = SVC(kernel=____, _____=gamma, random_state=10)  # TODO: Specify the correct kernel and parameter name for gamma\n",
        "\n",
        "    # Cross-validate the model\n",
        "    accuracy = np.mean(______(_____, X_train_scaled, y_train, cv=cv, scoring='accuracy'))  # TODO: Use cross-validation to compute the mean accuracy\n",
        "    precision = np.mean(cross_val_score(svm_rbf, ____, y_train, cv=cv, scoring='precision_weighted'))  # TODO: Provide the feature set and compute the mean precision\n",
        "    f1 = np.mean(cross_val_score(svm_rbf, X_train_scaled, y_train, ___=cv, scoring='f1_weighted'))  # TODO: Compute the mean F1 score using cross-validation\n",
        "\n",
        "    # Store the scores\n",
        "    accuracies.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Find the index and value of the highest precision\n",
        "max_precision_index = np.____(precisions)  # TODO: Find the index of the maximum precision\n",
        "max_precision_value = precisions[max_precision_index]\n",
        "max_precision_gamma = gamma_values[max_precision_index]\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(gamma_values, accuracies, label='Accuracy', marker='o')\n",
        "plt.plot(gamma_values, precisions, label='Precision', marker='o')\n",
        "plt.plot(gamma_values, f1_scores, label='F1 Score', marker='o')\n",
        "\n",
        "# Highlighting the point with highest precision\n",
        "plt.scatter(max_precision_gamma, max_precision_value, color='red')  # Highlight the point\n",
        "plt.axvline(x=max_precision_gamma, color='r', linestyle='--', lw=2)  # Vertical line\n",
        "plt.annotate(f'Highest Precision: {max_precision_value:.2f}\\nGamma: {max_precision_gamma}',\n",
        "             (max_precision_gamma, max_precision_value),\n",
        "             textcoords=\"offset points\", xytext=(0, -50), ha='center')\n",
        "\n",
        "plt.xlabel('Gamma')\n",
        "plt.ylabel('Score')\n",
        "plt.title('SVM Performance Comparison for Different Gamma Values')\n",
        "plt.xscale('log')  # Using logarithmic scale for gamma values\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9buDsOXw9RZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Optimal Regularization Parameter (C)\n",
        "\n",
        "#### Context:\n",
        "The regularization parameter \\( C \\) in Support Vector Machines (SVM) controls the trade-off between achieving a low error on the training data and minimizing the model complexity for better generalization. Determining the optimal \\( C \\) value is crucial for model performance, especially when using SVM for classification tasks.\n",
        "\n",
        "#### Task:\n",
        "Use Recursive Feature Elimination (RFE) to select the top 16 features for training an SVM classifier. Then, determine the optimal \\( C \\) value from a range of possible values to maximize the precision of the classifier.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Feature Selection:**\n",
        "   - Use RFE with SVM to reduce the number of features to the top 16 most significant features.\n",
        "   - Standardize the features before applying RFE.\n",
        "   \n",
        "2. **Model Training and Validation:**\n",
        "   - Train an SVM model with a linear kernel using the selected features.\n",
        "   - Explore a range of \\( C \\) values using logarithmic scaling to determine the best \\( C \\) for maximizing precision.\n",
        "   - Employ cross-validation using `StratifiedKFold` with 5 splits to evaluate model performance metrics such as accuracy, precision, and F1 score.\n",
        "\n",
        "3. **Performance Analysis:**\n",
        "   - Plot the performance metrics across different \\( C \\) values.\n",
        "   - Identify and highlight the \\( C \\) value that results in the highest precision.\n",
        "\n",
        "#### Question:\n",
        "Based on the evaluation, in which range does the optimal \\( C \\) value for achieving the highest precision fall?\n",
        "\n",
        "#### Options:\n",
        "(A) 0 to 1  \n",
        "(B) 1 to 3  \n",
        "(C) 3 to 6  \n",
        "(D) 6 to 10  \n",
        "\n",
        "#### Learning Point:\n",
        "This question demonstrates the importance of regularization in SVMs and helps students understand how the choice of \\( C \\) affects the model's ability to generalize. This knowledge is crucial for tuning models effectively, especially in scenarios prone to overfitting or underfitting.\n"
      ],
      "metadata": {
        "id": "gwCm5m4VB8fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the RFE with SVM as the estimator and select the top 16 features\n",
        "selector = RFE(SVC(_____='linear', random_state=42), ______=16, step=1)  # TODO: Specify the correct parameter names\n",
        "X_train_rfe = _____.fit_transform(X_train_scaled, y_train)  # TODO: Fit and transform the training data with RFE\n",
        "X_test_rfe = _____.transform(X_test_scaled)  # TODO: Transform the test data with RFE\n",
        "\n",
        "# Prepare cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Testing different C values\n",
        "c_values = np.logspace(-3, 2, 20)  # Explore a range of C values on a log scale\n",
        "accuracies = []\n",
        "precisions = []\n",
        "f1_scores = []\n",
        "\n",
        "for c in c_values:\n",
        "    svm_linear = ___(kernel='linear', ____=c, random_state=42)  # TODO: Initialize the SVC model with a linear kernel and the C parameter\n",
        "\n",
        "    # Cross-validate the model using only the selected features by RFE\n",
        "    accuracy = np.mean(cross_val_score(_____, X_train_rfe, y_train, cv=cv, ____='accuracy'))  # TODO: Use cross_val_score to compute the mean accuracy\n",
        "    precision = np.mean(cross_val_score(svm_linear, ____, y_train, cv=cv, scoring='precision_weighted'))  # TODO: Compute the mean precision\n",
        "    f1 = np.___(cross_val_score(svm_linear, X_train_rfe, ____, __=cv, scoring='f1_weighted'))  # TODO: Compute the mean F1 score\n",
        "\n",
        "    # Store the scores\n",
        "    accuracies.append(accuracy)\n",
        "    precisions.append(precision)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Find the index and value of the highest precision\n",
        "max_precision_index = np.____(precisions)  # TODO: Find the index of the maximum precision\n",
        "max_precision_value = precisions[max_precision_index]\n",
        "max_precision_c = c_values[max_precision_index]\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(c_values, accuracies, label='Accuracy', marker='o')\n",
        "plt.plot(c_values, precisions, label='Precision', marker='o')\n",
        "plt.plot(c_values, f1_scores, label='F1 Score', marker='o')\n",
        "\n",
        "# Highlighting the point with highest precision\n",
        "plt.scatter(max_precision_c, max_precision_value, color='red')  # Highlight the point\n",
        "plt.axvline(x=max_precision_c, color='r', linestyle='--', lw=2)  # Vertical line\n",
        "plt.annotate(f'Highest Precision: {max_precision_value:.2f}\\nC: {max_precision_c}',\n",
        "             (max_precision_c, max_precision_value),\n",
        "             textcoords=\"offset points\", xytext=(0, -50), ha='center')\n",
        "\n",
        "plt.xlabel('C Value')\n",
        "plt.ylabel('Score')\n",
        "plt.title('SVM with Linear Kernel Performance Comparison for Different C Values')\n",
        "plt.xscale('log')  # Using logarithmic scale for C values\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vVgB8eF1ANz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "### Counting Support Vectors in SVM\n",
        "\n",
        "#### Context:\n",
        "Support Vector Machine (SVM) is a robust classification technique that constructs a hyperplane or set of hyperplanes in a high-dimensional space, which can be used for classification, regression, or other tasks. A critical component of the SVM classifier is the support vectors, which are the data points nearest to the hyperplane and influence its position and orientation.\n",
        "\n",
        "#### Task:\n",
        "Use t-Distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction followed by SVM to classify data. Determine the total number of support vectors involved in classifying the data.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Data Preparation and Preprocessing:**\n",
        "   - Filter the dataset for two specific classes for simplification.\n",
        "   - Apply standard scaling to the features to normalize data, enhancing the SVM's performance.\n",
        "\n",
        "2. **Dimensionality Reduction:**\n",
        "   - Implement t-SNE to reduce feature dimensions to two components, which aids in visualizing the dataset.\n",
        "\n",
        "3. **SVM Training:**\n",
        "   - Train an SVM classifier using the t-SNE output. Set the kernel to linear and regularization parameter \\( C \\) to 1.0.\n",
        "   - Identify the support vectors from the trained model.\n",
        "\n",
        "4. **Visualization:**\n",
        "   - Plot the decision boundaries and highlight the support vectors.\n",
        "   - Use contours to depict decision boundaries and margins.\n",
        "\n",
        "5. **Analysis:**\n",
        "   - Report the total number of support vectors found.\n",
        "\n",
        "\n",
        "#### Question:\n",
        "After training the SVM model on t-SNE reduced features, how many support vectors are found?\n",
        "\n",
        "#### Options:\n",
        "(A) 10  \n",
        "(B) 15  \n",
        "(C) 4  \n",
        "(D) 20  \n",
        "\n",
        "#### Learning Point:\n",
        "This question helps students visualize complex machine learning concepts like SVM decision boundaries and the role of support vectors in high-dimensional space, providing insight into the mechanics of SVM classifiers and the effectiveness of dimensionality reduction techniques like t-SNE.\n",
        "\n"
      ],
      "metadata": {
        "id": "VvXtf8XD9tKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# List of features\n",
        "features = ['AREA', 'PERIMETER', 'MAJOR_AXIS', 'MINOR_AXIS', 'ECCENTRICITY',\n",
        "       'EQDIASQ', 'SOLIDITY', 'CONVEX_AREA', 'EXTENT', 'ASPECT_RATIO',\n",
        "       'ROUNDNESS', 'COMPACTNESS', 'SHAPEFACTOR_1', 'SHAPEFACTOR_2',\n",
        "       'SHAPEFACTOR_3', 'SHAPEFACTOR_4', 'MeanRR', 'MeanRG', 'MeanRB',\n",
        "       'StdDevRR', 'StdDevRG', 'StdDevRB', 'SkewRR', 'SkewRG', 'SkewRB',\n",
        "       'KurtosisRR', 'KurtosisRG', 'KurtosisRB', 'EntropyRR', 'EntropyRG',\n",
        "       'EntropyRB', 'ALLdaub4RR', 'ALLdaub4RG', 'ALLdaub4RB']\n",
        "\n",
        "# Create a copy of the DataFrame\n",
        "df_temp = df.copy()\n",
        "\n",
        "# Select features and filter for two classes\n",
        "df_temp['Class'] = df_temp['Class'].apply(lambda x: 'SAFAVI' if x == 'SAFAVI' else 'Other')\n",
        "\n",
        "# Encoding classes\n",
        "df_temp['Label'] = df_temp['Class'].____({'SAFAVI': 1, 'Other': 0})  # TODO: Use the appropriate method to map class labels to numerical values\n",
        "\n",
        "# Standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_temp[features])\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "# SVM training\n",
        "svm = ____(kernel='linear', ___=1.0)  # TODO: Initialize the SVC model with the correct parameters\n",
        "svm.fit(X_tsne, df_temp['Label'])\n",
        "\n",
        "# Support vectors\n",
        "# Visit https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html to get the correct attribute for support vectors\n",
        "support_vectors = svm.______\n",
        "\n",
        "print(f'---------------------------------------------------------------')\n",
        "print(f'Total Number of Support Vectors found are {len(support_vectors)}')\n",
        "print(f'---------------------------------------------------------------')\n",
        "\n",
        "# Create grid to plot decision boundaries\n",
        "x_min, x_max = X_tsne[:, 0].min() - 1, X_tsne[:, 0].max() + 1\n",
        "y_min, y_max = X_tsne[:, 1].min() - 1, X_tsne[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Plot decision boundary and margins\n",
        "Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])  # TODO: Use the decision function to predict decision boundaries\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.figure(figsize=(10, 8))  # Define plot size here to ensure it encompasses all the elements\n",
        "plt.contourf(xx, yy, Z, levels=[-1, 0, 1], alpha=0.2, colors=['blue', 'grey', 'red'])  # Background color for decision areas\n",
        "plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])  # Decision boundaries and margins\n",
        "\n",
        "# Plot data points and support vectors\n",
        "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df_temp['Label'], cmap=ListedColormap(['#FF0000', '#0000FF']), s=50, edgecolors='k')\n",
        "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=120, facecolors='none', edgecolors='yellow', linewidths=2, label='Support vectors')\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel('t-SNE Feature 1')\n",
        "plt.ylabel('t-SNE Feature 2')\n",
        "plt.title('t-SNE Visualization with SVM Decision Boundary for Date Fruit Classification')\n",
        "plt.legend(handles=[scatter], labels=['Data points'], loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c9jDpKkjxziF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Implement Hinge Loss from Scratch\n",
        "\n",
        "#### Context:\n",
        "Support Vector Machines (SVMs) rely on the hinge loss function to penalize misclassified points and those that are too close to the decision boundary. Understanding hinge loss helps explain how SVMs work.\n",
        "\n",
        "#### Task:\n",
        "Write a function to compute hinge loss for a simple binary classification dataset and use it to determine the loss of a sample classifier.\n",
        "\n",
        "#### Instructions:\n",
        "1. **Define Hinge Loss Function**: Implement a Python function to compute hinge loss using the following steps:\n",
        "   - **Step 1**: Calculate the margin for each data point: \\( $\\text{margin} = y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b))$.\n",
        "   - **Step 2**: Compute the hinge loss for each data point: \\( $\\max(0, 1 - \\text{margin}) \\$).\n",
        "   - **Step 3**: Return the average hinge loss across all data points.\n",
        "2. **Evaluate Sample Classifier**: Use your function to compute the hinge loss for a classifier defined by weights and bias.\n",
        "3. **Answer the Question**: Based on the computed loss, answer the following question about the nature of hinge loss and its implications.\n",
        "\n",
        "#### Question:\n",
        "What does the computed hinge loss value indicate about the classifier's performance?\n",
        "\n",
        "**Statements**:\n",
        "1. The hinge loss value \\( > 1 \\) indicates that most points are misclassified or too close to the decision boundary.\n",
        "2. The hinge loss value \\( < 1 \\) suggests that most points are classified correctly but may still be close to the decision boundary.\n",
        "3. The hinge loss value \\( = 0 \\) indicates that all points are classified correctly and far from the decision boundary.\n",
        "4. The hinge loss value \\( = 1 \\) suggests that all points are classified correctly.\n",
        "\n",
        "**Options**:\n",
        "\n",
        "A) Statement 1  \n",
        "B) Statement 2  \n",
        "C) Statement 3  \n",
        "D) Statement 4  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OxwXA9IeFdR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Data (Binary classification)\n",
        "X = np.array([[2, 3], [3, 4], [1, 1], [4, 5], [6, 7]])\n",
        "y = np.array([1, 1, -1, 1, -1])\n",
        "\n",
        "data = pd.DataFrame(X, columns = ['f1', 'f2'])\n",
        "data['y'] = y\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "mi97IrCEP2gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hinge Loss Implementation\n",
        "def hinge_loss(X, y, weights, bias):\n",
        "    \"\"\"\n",
        "    Compute hinge loss for a simple SVM model.\n",
        "    Args:\n",
        "        X (ndarray): Feature matrix.\n",
        "        y (ndarray): Target vector.\n",
        "        weights (ndarray): Weight vector.\n",
        "        bias (float): Bias term.\n",
        "    Returns:\n",
        "        float: Hinge loss value.\n",
        "    \"\"\"\n",
        "    margins = y * (np.___(X, ____) + ___)  # TODO: Calculate the margins using a dot product of weights and features, and add the bias\n",
        "    hinge_loss = np.____(0, 1 - ____)  # TODO: Apply the hinge loss formula: max(0, 1 - margin)\n",
        "    return np.____(hinge_loss)  # TODO: Return the mean or sum of the hinge loss values\n",
        "\n",
        "# Example weights and bias\n",
        "weights = np.array([0.5, 0.5])\n",
        "bias = 0.0\n",
        "\n",
        "# Compute Hinge Loss\n",
        "loss = hinge_loss(X, y, weights, bias)\n",
        "print(f'Hinge Loss: {loss:.4f}')"
      ],
      "metadata": {
        "id": "cqAoxn7hPHFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}